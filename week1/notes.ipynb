{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frontier Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed source models\n",
    "\n",
    "- ChatGPT from OpenAI\n",
    "- Claude from Anthropic\n",
    "- Gemini from Google\n",
    "- Command R from Cohere\n",
    "- Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open source models\n",
    "\n",
    "- Llama from Meta\n",
    "- Mixtral from Mistral\n",
    "- Qwen from Alibaba Cloud\n",
    "- Gemma from Google\n",
    "- Phi from Microsoft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three way to use models\n",
    "\n",
    "- Chat interfaces\n",
    "- Cloud API\n",
    "    - Frameworks like Langchain\n",
    "    - Managed AI cloud services\n",
    "        - Amazon Bedrock\n",
    "        - Google Vertex\n",
    "        - Azure ML\n",
    "- Direct inference\n",
    "    - Huggingface Transformers library\n",
    "    - Ollama locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journey so far\n",
    "\n",
    "1. Prompt Engineer\n",
    "2. Custom GPTs\n",
    "3. Copilots\n",
    "4. Agentization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens\n",
    "\n",
    "- In the early days, neural networks were trained at the character level\n",
    "    - Predict the next character in this sequence\n",
    "    - Small vocab, but expects too much from the network\n",
    "- Then neural networks were trained off words\n",
    "    - Predict the next word in this sequence\n",
    "    - Much easier to learn from, but leads to enormous vocabs with rare words omitted\n",
    "- The breakthrough was to work with chunks of words, called 'tokens'\n",
    "    - A middle ground: manageable vocab, and useful information for the neural network\n",
    "    - In addition, elegantly handles word stems\n",
    "\n",
    "## GPT Tokenizer\n",
    "\n",
    "<img src=\"./assets/Screenshot%202025-07-23%20at%208.57.48 PM.png\" width=\"450\"/>\n",
    "<img src=\"./assets/Screenshot 2025-07-23 at 9.05.00 PM.png\" width=\"450\"/>\n",
    "<img src=\"./assets/Screenshot%202025-07-23%20at%208.58.02 PM.png\" width=\"450\"/>\n",
    "\n",
    "### General Rule of thumb\n",
    "\n",
    "- 1 token = ~ 4 characters\n",
    "- 1 token = ~ 0.75 word\n",
    "- 1000 tokens = 750 words\n",
    "\n",
    "--\n",
    "\n",
    "- Digits are tokenized in groups of 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Window\n",
    "\n",
    "- The max number of tokens that model can consider when generating the next token\n",
    "- Includes original prompt, subsequent conversations, the latest input prompt and almost all the output prompts\n",
    "- Governs how model can remember references, content and context"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
